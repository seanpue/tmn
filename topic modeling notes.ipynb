{
 "metadata": {
  "name": "",
  "signature": "sha256:be4c1bbb69921f99aaa2f106dffe01680bd248d07e772cd105753c4e0c91a98d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Notes on \"Probablistic Topic Modeling\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*probabilistic topic modeling*: \"a suite of algoirthms that aim to discover and annotate large archives of documents with thematic information\"\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## *latent Dirichlet allocation (LDA)* -\u00a0simplest topic model\n",
      "\n",
      "* a statistical model of document collections that tries to capture the intuition that documents exhibit multiple topics\n",
      "* a generative process: the imaginary random process by which the model assumes the documents arose\n",
      "* assumes topics generated first\n",
      "* a generative probabilistic model for collections of discrete data such as text corpora\n",
      "* a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics\n",
      "* developed to fix an issue with *probabilistic latent semantic analysis* (pLSI), a probabilistic version of *latent semantic analysis*\n",
      "* can be seen as a type of principle component analysis for discrete data\n",
      "\n",
      "\n",
      "## *topic*: a distribution over a fixed vocabulary\n",
      "\n",
      "* generate words in document\n",
      "  * randomly choose a distribution over topics (*each document exhibits topics in different proportion)\n",
      "  * for each word in the document\n",
      "    * randomly choose a topic from the distribution over topics in step #1 (selected topic is chose from the per-document distribution over topics\n",
      "    * randomly choose a word from the corresponding distribution over the vocabulary (each word in the document is drawn from one of the topics)\n",
      "* all documents in collection share same set of topics, but exibits in different proportion (distribution over topics would place probabilty on)\n",
      "* modeled as an infinite mixture over an underlying set of topic probabilities\n",
      "* topic probabilities provide an explicit representation of a document\n",
      "* defines a multinomial distribution over the vocabulary and is assumed to have been drawn for a Dirichlet, $Beta_k ~ Dirichlet($\\eta)\n",
      "\n",
      "## hidden structure\n",
      "\n",
      "* the topics, per-document topic distributions, per-document per-word topic assignments\n",
      "* inferred hidden structure resembles the thematic structure of the collection\n",
      "\n",
      "## generative probabilistic modeling\n",
      "* treat data as arising form a generative process that includes *hidden variables*\n",
      "* defines a *joint probability distribution* over both observed and hidden random variables\n",
      "* data analysis by using that joint distribution to compute the *conditional distribution* of the hidden variables given observed varaibles\n",
      "* conditional distribution* also called *posterior distribution*\n",
      "\n",
      "* observed variables = words of documents\n",
      "* hidden variables = topic structure\n",
      "* generative process is as described here\n",
      "\n",
      "## computational problem\n",
      "* computational problem of inferring the hidden topic stricture from the documents = the problem of computing the posterior distribution (the conditional distribution of the hidden variables given the documents\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## LDA formally defined\n",
      "\n",
      "generative process for LDA corresponds to the following joint distribution of the hidden and observed variables\n",
      "\n",
      "$$p( \\beta_{1:K} ,\\theta_{1:D} , z_{1:D} , w_{1:D} ) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$=\\Pi_{i=1}^{K}p(B_i)\\Pi_{d=1}^{D}p(\\theta_d)\\big( \\Pi_{n=1}^{N}p(z_{d,n}|\\theta_d)p(w_{d,n}|B_{1:K},z_{d,n})\\big)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$K$ = number of topics\n",
      "\n",
      "$\\beta_{1:K}$ = topics\n",
      "\n",
      "$\\beta_k$ = distribution over the vocabulary\n",
      "\n",
      "$\\theta_d$ = topic proportions for *d*th document\n",
      "\n",
      "$\\theta_{d,k}$ = topic proportion for topic *k* in document *d*\n",
      "\n",
      "$z_d$ = topic assignments for *d*th document\n",
      "\n",
      "$z_{d,n}$ = topic assignment for *n*th word in document *d*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note dependencies: \n",
      "* $z_{d,n}|\\theta_d$ : topic assignment ($z_{d,n}$) is dependent on per-document topic proportions \\theta_d\n",
      "* $w_{d,n}|\\beta_{1:K},z_{d,n}$: observed words $w_{d,n}$ depends on topic assignment ($z_{d,n}$) and *all* topics ($\\beta_{1:K}$)\n",
      "  * looking up th as to which topic $z_{d,n}$ refers to and looking up the probability of the word $w_{d,n}$ within that topic\n",
      "  * these dependencies define LDA\n",
      "    * encoded in statistical assuptions behind generative process\n",
      "    * in the particularly mathemetical form of the joint distribution\n",
      "    * in the *probabilistic graphical model* for LD\n",
      "      * a graphical language for describing families of probability distributions\n",
      "      \n",
      "  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Posterior Computation for LDA\n",
      "\n",
      "* problem: computing the conditional distribution of topic structure given the observed documents\n",
      "    \n",
      "$$p( \\beta_{1:K} ,\\theta_{1:D} , z_{1:D} , w_{1:D} ) $$\n",
      "$$= \\frac{p(\\beta_{1:K} ,\\theta_{1:D} , z_{1:D} , w_{1:D}) }{p(w_{1:D})}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$p( \\beta_{1:K} ,\\theta_{1:D} , z_{1:D} , w_{1:D} ) $ = joint distribution of all the random variables\n",
      "$p(w_{1:D})$ = *marginal probability* of the observation\n",
      "* probability of seeing the observed corpus under any topic model\n",
      "* theroeticaly, can be computed by summing joint distribution over very possible instantiation of the hidden topic structure\n",
      "  * exponentially large; intractable to compute\n",
      "  * cannot compute posterior because of the denominator (the *evidence*)\n",
      "    * try to approximate it\n",
      "      * topic modeling algorithms are adaptations of general-purpose methods for approximating the posterior distribution\n",
      "\n",
      "* topic modeling algorithms form an **approximation** of Equation 2 by adapting an alternative distribution over the latent topic structure to be close to the true posterior\n",
      "  * tma can be sampling-based or variational\n",
      "    * **sampling based**: collect samples from the posterior to approximate it with an empirical distribution\n",
      "      * for topic modeling, *Gibbs sampling* where we construct a *Markov chain*\n",
      "        * a sequence of random variables\n",
      "          * each dependent on the previous\n",
      "          * limiting distribution is the posterior\n",
      "        * defined on the hidden topic variables for a particular corpus,\n",
      "        * Markov Chain algorithm:\n",
      "          * run the chain for a long time\n",
      "          * collect samples from the limiting distribution\n",
      "          * approximate distribution with collected samples\n",
      "            * See Steyvers and Griffiths for description\n",
      "            * CRAN.R-project.org/package=lda\n",
      "    * **variational**: deterministic alternative\n",
      "      * rather than approximating the posterior with samples\n",
      "        * posits a parameterized family of distributions over the hidden structure\n",
      "        * finds the member of that family that is closest to the postrior\n",
      "        * inference problem --> optimization problem\n",
      "            * allow for innovation in optimization to have practical impact in probabilistic modeling\n",
      "            * Blei et al.: a coordinate ascent variational inference algorithm for LDA\n",
      "            * Hoffman et al: faster online algorithm\n",
      "            "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Algorithm: According to Mimno:\n",
      "\n",
      "* initialize topic assignments randomly\n",
      "* for each iteration:\n",
      "  * for each document\n",
      "    * for each word:\n",
      "      * resample a topic for word, given all other words and their current topic assignments\n",
      "        * remove it from where it currently is\n",
      "        * consider: which topics occur in this document?\n",
      "          * even though topic is not in document, still give it a little value (hyperparameter); no zeros\n",
      "          * which topics like the word?\n",
      "          * multiply: get how much document likes topic (wide) * how much topic likes word (long)\n",
      "          * then throw at dart board; see what it hits; then assign word to topic - lengthens width and height for that word's weight\n",
      "          * even if you start randomly, each time you assign a word (due to slight imbalance) it's more likely the next time it will go the same way \u2014 This is Gibb sampling\n",
      "          * vs. variational inference (2 hours); instead of one # a probability distribution \u00a0\u2014 waights taken from something similar\n",
      "          \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is a document?\n",
      "* depends how you segment it\n",
      "\n",
      "Which words are interesting? \n",
      "* multiwordterms vs. single tokens\n",
      "\n",
      "What is a word, anyway?\n",
      "\n",
      "Knobs to set:\n",
      "* number of topics\n",
      "* hyper-parameters\n",
      "\n",
      "alpha: # of topics * value setting from command line; a value adding to # of topics in document. \n",
      "alpha: smoothing constant\n",
      "\n",
      "hyper parameters can be optimized; so it learns what the best parameter is for each topic.\n",
      "\n",
      "fixed: all topics similar size, quality; cons: duplicate topics, frequent words repeaed \n",
      "learned: some topics big, others small; small topics may be low quality\n",
      "\n",
      "diagonistics:\n",
      "\n",
      "What makes bad topics?\n",
      "* random, unrelated words\n",
      "* one or two 'intruder' words\n",
      "* boring, overly general\n",
      "* two or more good topics combined, sometimes with a general word in topic (chimaeras)\n",
      "  * ben schmidt\n",
      " \n",
      "Evaluations of topic quality:\n",
      "* size (# of tokens assigned)\n",
      "* within-doc rank\n",
      "* similarity to corpus-wide distribution\n",
      "* locally-frequent words\n",
      "* co-doc coherence\n",
      "\n",
      "visualization: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}